{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "gwggq-CL9cdi",
      "metadata": {
        "id": "gwggq-CL9cdi"
      },
      "source": [
        "# Lab 0: Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d813c76c",
      "metadata": {
        "id": "d813c76c"
      },
      "source": [
        "**Objective**: Set up the development environment for building AI agents\n",
        "\n",
        "In this lab, you will:\n",
        "1. Install required Python packages\n",
        "2. Set up GitHub Models for free LLM access\n",
        "3. Deploy the mock backend using ngrok\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pAPA6lT_9jVp",
      "metadata": {
        "id": "pAPA6lT_9jVp"
      },
      "source": [
        "## Step 1: Install Required Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8604f2",
      "metadata": {
        "id": "6b8604f2"
      },
      "source": [
        "We'll install all necessary Python packages for agent development.\n",
        "\n",
        "> You may see an error message like this at the bottom, but that's okay, as long as the call is successfully excuted (there a green tick next to the cell on the left):\n",
        "> ![image.png](https://i.ibb.co/nFVsYs4/python-package-download-error.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ff58cace",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ff58cace",
        "outputId": "055acb81-47ad-4355-ff59-75860eb1935a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.9/88.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m217.9/217.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m338.1/338.1 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m221.0/221.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.3/213.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m197.2/197.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m275.4/275.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.5/122.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.2/131.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.6/111.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.2/143.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m429.0/429.0 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m463.4/463.4 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install all required packages\n",
        "!pip install -q agent-framework --pre requests fastapi uvicorn pyngrok nest-asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90Bk1_4k-UB7",
      "metadata": {
        "id": "90Bk1_4k-UB7"
      },
      "source": [
        "## Step 2: Configure GitHub Models Access with Colab Secrets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92619753",
      "metadata": {
        "id": "92619753"
      },
      "source": [
        "Instructions to Store Your GitHub Token Securely:\n",
        "\n",
        "**Step 2a: Get Your GitHub Personal Access Token (PAT)**\n",
        "1. Visit https://github.com/settings/tokens?type=beta\n",
        "2. Click \"Generate new token\" (fine-grained token)\n",
        "3. Set token name (e.g., \"GitHub Models Access\")\n",
        "4. Set expiration (e.g., 90 days)\n",
        "5. Under \"Permissions\", select \"Read access to content\"\n",
        "6. Click \"Generate token\" and **COPY THE TOKEN**\n",
        "\n",
        "**Step 2b: Store in Colab Secrets (Recommended)**\n",
        "1. Look at the left sidebar of this Colab notebook\n",
        "2. Click the **ğŸ”‘ key icon** (Secrets)\n",
        "3. Click **\"+ Add new secret\"**\n",
        "4. Name: `GITHUB_PAT`\n",
        "5. Value: **Paste your token here**\n",
        "6. Toggle **\"Notebook access\"** to ON\n",
        "\n",
        "![Colab Secrets Location](https://colab.research.google.com/img/colab_favicon_256px.png)\n",
        "\n",
        "**Why use Secrets?**\n",
        "- Tokens are encrypted and not visible in notebook cells\n",
        "- Prevents accidental token exposure when sharing notebooks\n",
        "- Follows security best practices\n",
        "\n",
        "**Note**: GitHub Models provides free access to various LLMs for development purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c7450376",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7450376",
        "outputId": "f372dac8-f78a-4214-a25e-65c9ce41e94d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… GitHub PAT loaded from Colab Secrets successfully!\n",
            "âœ… Token length: 93 characters\n"
          ]
        }
      ],
      "source": [
        "# Load GitHub token from Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    GITHUB_PAT = userdata.get('GITHUB_PAT')\n",
        "    print(\"âœ… GitHub PAT loaded from Colab Secrets successfully!\")\n",
        "    print(f\"âœ… Token length: {len(GITHUB_PAT)} characters\")\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ WARNING: Could not load GITHUB_PAT from Colab Secrets!\")\n",
        "    print(\"   Please follow Step 2b above to add your token to Secrets.\")\n",
        "    print(f\"   Error: {e}\")\n",
        "    GITHUB_PAT = \"\"  # Fallback"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IgYt3V6f-hM_",
      "metadata": {
        "id": "IgYt3V6f-hM_"
      },
      "source": [
        "## Step 3: Test GitHub Models Connection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daf5ea3a",
      "metadata": {
        "id": "daf5ea3a"
      },
      "source": [
        "We'll test that we can connect to GitHub Models using the OpenAI Chat Completions API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0ydLaWlF2WtL",
      "metadata": {
        "id": "0ydLaWlF2WtL"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "JX928WpA2VmB",
      "metadata": {
        "id": "JX928WpA2VmB"
      },
      "outputs": [],
      "source": [
        "# Initialize OpenAI chat client with GitHub Models endpoint\n",
        "client = OpenAI(\n",
        "  base_url=\"https://models.github.ai/inference\",\n",
        "  api_key=GITHUB_PAT,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "DTdKvs-v2aH1",
      "metadata": {
        "id": "DTdKvs-v2aH1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "6edf8958-1ca3-44f4-83f6-05529a60b030"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "PermissionDeniedError",
          "evalue": "Error code: 403 - {'error': {'code': 'no_access', 'message': 'No access to model: openai/gpt-4o-mini', 'details': 'No access to model: openai/gpt-4o-mini'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2618937692.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"openai/gpt-4o-mini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     messages=[\n\u001b[1;32m      4\u001b[0m         {\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1188\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPermissionDeniedError\u001b[0m: Error code: 403 - {'error': {'code': 'no_access', 'message': 'No access to model: openai/gpt-4o-mini', 'details': 'No access to model: openai/gpt-4o-mini'}}"
          ]
        }
      ],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"What is in the meaning of life?\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    max_tokens=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new-cell-1",
        "outputId": "6d4516ca-ceae-4583-db1b-9d32484c713c"
      },
      "source": [
        "# Upgrade the openai library to a compatible version\n",
        "!pip install --upgrade openai\n",
        "\n",
        "# After running this cell, please re-run the previous cell (DTdKvs-v2aH1)\n",
        "# and then the following cell (f11f6bdf) to test the fix."
      ],
      "id": "new-cell-1",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f11f6bdf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "f11f6bdf",
        "outputId": "08e1c8ba-1a5e-442a-d46c-f05bcf8cb8b6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'completion' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6241522.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'completion' is not defined"
          ]
        }
      ],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UgjOg_sl-lxK",
      "metadata": {
        "id": "UgjOg_sl-lxK"
      },
      "source": [
        "## Step 4: Download & Deploy Mock Backend with ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14728f6",
      "metadata": {
        "id": "e14728f6"
      },
      "source": [
        "We'll download the FastAPI backend from the repository and deploy it using ngrok to create a public URL.\n",
        "\n",
        "**What happens here:**\n",
        "- Downloads `mock_backend.py` from GitHub to Colab's `/content/` directory (default working directory)\n",
        "- Starts the FastAPI server in the background (runs on port 8000)\n",
        "- Creates a public URL via ngrok for external access\n",
        "- You can access the file at `/content/mock_backend.py` if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zf9k7OhG-ywH",
      "metadata": {
        "id": "Zf9k7OhG-ywH"
      },
      "source": [
        "### Step 4a: Download & Save the Mock Backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OBsHDQkC6rJc",
      "metadata": {
        "id": "OBsHDQkC6rJc"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "szYseR4v6u9w",
      "metadata": {
        "id": "szYseR4v6u9w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "e4eb3907-0ace-464c-a634-55eec6eb15be"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nest_asyncio' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-889091541.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Allow nested event loops (required for Colab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Download the mock backend from GitHub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbackend_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/tezansahu/building-eval-driven-ai-agents/main/backend/mock_backend.py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nest_asyncio' is not defined"
          ]
        }
      ],
      "source": [
        "# Allow nested event loops (required for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Download the mock backend from GitHub\n",
        "backend_url = \"https://raw.githubusercontent.com/tezansahu/building-eval-driven-ai-agents/main/backend/mock_backend.py\"\n",
        "\n",
        "print(\"ğŸ“¥ Downloading mock backend from repository...\")\n",
        "response = requests.get(backend_url)\n",
        "\n",
        "# Save to /content/ directory (Colab's default working directory)\n",
        "with open('mock_backend.py', 'w') as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "print(\"âœ… Backend code downloaded to /content/mock_backend.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "jkMD3K9f7D-J",
      "metadata": {
        "id": "jkMD3K9f7D-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "479fc720-648a-47d9-c6ce-008b1b91b67a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mock_backend'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3928400265.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import the backend module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmock_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mock_backend'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import the backend module\n",
        "import mock_backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YmpeWcMG7HZG",
      "metadata": {
        "id": "YmpeWcMG7HZG"
      },
      "outputs": [],
      "source": [
        "# Start server in background thread\n",
        "print(\"ğŸš€ Starting FastAPI server...\")\n",
        "server_thread = mock_backend.run_in_thread(port=8000)\n",
        "time.sleep(2)  # Give server time to start\n",
        "\n",
        "print(\"âœ… FastAPI server started on port 8000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hr3wjVIAH6NS",
      "metadata": {
        "id": "Hr3wjVIAH6NS"
      },
      "source": [
        "### Step 4b: Configure ngrok Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bA345Ie77PA",
      "metadata": {
        "id": "0bA345Ie77PA"
      },
      "source": [
        "**ngrok requires a free account and authtoken for usage.**\n",
        "\n",
        "**Instructions:**\n",
        "1. Go to https://dashboard.ngrok.com/signup and create a free account\n",
        "2. After signup, go to https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "3. Copy your authtoken\n",
        "4. In Colab, click the **ğŸ”‘ key icon** (Secrets) on the left sidebar\n",
        "5. Click **\"+ Add new secret\"**\n",
        "6. Name: `NGROK_AUTHTOKEN`\n",
        "7. Value: **Paste your ngrok authtoken**\n",
        "8. Toggle **\"Notebook access\"** to ON\n",
        "\n",
        "**Why ngrok?** It creates a public URL for our local backend so the agent can access it from anywhere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MMdS_ayA8SpE",
      "metadata": {
        "id": "MMdS_ayA8SpE"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    NGROK_AUTHTOKEN = userdata.get('NGROK_AUTHTOKEN')\n",
        "    ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "    print(\"âœ… ngrok authenticated successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ WARNING: Could not load NGROK_AUTHTOKEN from Colab Secrets!\")\n",
        "    print(\"   Please follow the instructions above to add your ngrok authtoken.\")\n",
        "    print(f\"   Error: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "euaGcBppH_Me",
      "metadata": {
        "id": "euaGcBppH_Me"
      },
      "source": [
        "### Step 4c: Expose the Server Publicly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z9lhDL0j7SL4",
      "metadata": {
        "id": "z9lhDL0j7SL4"
      },
      "outputs": [],
      "source": [
        "# Create public URL with ngrok\n",
        "print(\"ğŸŒ Creating public URL with ngrok...\")\n",
        "public_url = ngrok.connect(8000)\n",
        "BACKEND_URL = public_url.public_url\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"âœ… Backend server is running!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"ğŸ“¡ Public URL: {BACKEND_URL}\")\n",
        "print(f\"ğŸ“š API Docs: {BACKEND_URL}/docs\")\n",
        "print(f\"ğŸ“‚ File location: /content/mock_backend.py\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nâš ï¸ IMPORTANT: Copy the Public URL above!\")\n",
        "print(f\"   You'll need it for Lab 1 & Lab 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rgIWdpEzIkTq",
      "metadata": {
        "id": "rgIWdpEzIkTq"
      },
      "source": [
        "## Step 5: Test Backend API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "111d2c25",
      "metadata": {
        "id": "111d2c25"
      },
      "source": [
        "Let's verify the backend is working correctly by testing the endpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12788010",
      "metadata": {
        "id": "12788010"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wx06nhRA8jOV",
      "metadata": {
        "id": "Wx06nhRA8jOV"
      },
      "outputs": [],
      "source": [
        "# Test 1: Health check\n",
        "response = requests.get(f\"{BACKEND_URL}/\")\n",
        "print(\"Health Check:\", response.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Icc0Aqaq8lR6",
      "metadata": {
        "id": "Icc0Aqaq8lR6"
      },
      "outputs": [],
      "source": [
        "# Test 2: List events\n",
        "response = requests.get(f\"{BACKEND_URL}/events\")\n",
        "events = response.json()\n",
        "print(f\"\\nFound {len(events)} events.:\")\n",
        "for event in events:\n",
        "    print(f\"  - {event['name']} on {event['date']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N6kMi7FOIn1l",
      "metadata": {
        "id": "N6kMi7FOIn1l"
      },
      "source": [
        "## ğŸ‰ Lab 0 Complete!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-frXqnj-IryV",
      "metadata": {
        "id": "-frXqnj-IryV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "81770c32",
      "metadata": {
        "id": "81770c32"
      },
      "source": [
        "\n",
        "\n",
        "### What You Accomplished:\n",
        "\n",
        "âœ… **Installed Microsoft Agents Framework** - A high-level library for building AI agents  \n",
        "âœ… **Configured GitHub Models** - Free LLM access using GitHub PAT  \n",
        "âœ… **Stored secrets securely** - Using Colab Secrets (best practice)  \n",
        "âœ… **Deployed mock backend** - Campus event management API with ngrok  \n",
        "âœ… **Tested the setup** - Verified agent can call LLM and backend APIs\n",
        "\n",
        "### Key Concepts Learned:\n",
        "\n",
        "1. **Colab Secrets**: Secure way to store API tokens without exposing them in code\n",
        "2. **GitHub Models**: Free access to GPT-4o-mini for development\n",
        "3. **ngrok**: Makes local services accessible via public URL\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "**Lab 1**: Build a complete campus event agent with 3 tools:\n",
        "- Register students for events\n",
        "- Book venues for clubs\n",
        "- Send notifications to participants\n",
        "\n",
        "**Lab 2**: Evaluate and improve the agent using metrics:\n",
        "- Relevance Evaluator (Azure AI)\n",
        "- Task Adherence Evaluator (Azure AI)\n",
        "- Custom Conciseness Evaluator\n",
        "- Measure improvement systematically\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YwfP6haEIyZy",
      "metadata": {
        "id": "YwfP6haEIyZy"
      },
      "source": [
        "## Troubleshooting:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yth8FXHQI0ga",
      "metadata": {
        "id": "yth8FXHQI0ga"
      },
      "source": [
        "**Issue**: \"Could not load GITHUB_PAT from Colab Secrets\"\n",
        "- **Solution**: Click ğŸ”‘ icon â†’ Add new secret â†’ Name: `GITHUB_PAT` â†’ Enable notebook access\n",
        "\n",
        "**Issue**: \"ngrok tunnel failed\"\n",
        "- **Solution**: Re-run the ngrok cell. Tunnels expire after inactivity.\n",
        "\n",
        "**Issue**: Backend API not responding\n",
        "- **Solution**: Check that BACKEND_URL is set correctly and starts with `https://`"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}